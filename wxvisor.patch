diff --git a/Makefile b/Makefile
index 863f5850..aa9cdbc9 100644
--- a/Makefile
+++ b/Makefile
@@ -2,7 +2,7 @@
 VERSION = 4
 PATCHLEVEL = 18
 SUBLEVEL = 0
-EXTRAVERSION =
+EXTRAVERSION = wx_v2
 NAME = Merciless Moray
 
 # *DOCUMENTATION*
diff --git a/arch/arm/include/asm/ptdump.h b/arch/arm/include/asm/ptdump.h
index 3ebf9718..1541910b 100644
--- a/arch/arm/include/asm/ptdump.h
+++ b/arch/arm/include/asm/ptdump.h
@@ -36,6 +36,8 @@ void ptdump_check_wx(void);
 
 #ifdef CONFIG_DEBUG_WX
 #define debug_checkwx() ptdump_check_wx()
+void ptdump_check_wx_curr(struct task_struct *p);
+#define debug_checkwx() ptdump_check_wx_curr(current)
 #else
 #define debug_checkwx() do { } while (0)
 #endif
diff --git a/arch/arm64/boot/uImage b/arch/arm64/boot/uImage
index 71debb37..654ed345 100644
Binary files a/arch/arm64/boot/uImage and b/arch/arm64/boot/uImage differ
diff --git a/arch/arm64/include/asm/hypsec_mmu.h b/arch/arm64/include/asm/hypsec_mmu.h
index 1aa6d2d1..5f28f0e4 100644
--- a/arch/arm64/include/asm/hypsec_mmu.h
+++ b/arch/arm64/include/asm/hypsec_mmu.h
@@ -30,7 +30,8 @@ struct vring_data {
 
 struct s2_page {
 	int count;
-	u32 vmid;
+	u16 vmid;
+	u16 wx; // w_only: 0, x_only: 1
 	u64 gfn;
 };
 
@@ -78,6 +79,9 @@ extern void el2_flush_icache_range(unsigned long start, unsigned long end);
 void grant_stage2_sg_gpa(u32 vmid, u64 addr, u64 size);
 void revoke_stage2_sg_gpa(u32 vmid, u64 addr, u64 size);
 void set_balloon_pfn(struct shadow_vcpu_context *shadow_ctxt);
+void s2_page_text_wx_map(u32 vmid, u64 gpa);
+void s2_page_ktext_wx_map(u32 vmid, u64 gpa);
+void reset_pfn_wx(u64 pfn);
 
 void* alloc_stage2_page_split(u32 vmid, unsigned int num);
 void* alloc_stage2_page(unsigned int num);
diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index a46fd040..c24a5a64 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -393,6 +393,9 @@ struct kvm_vcpu_stat {
 	u64 mmio_exit_user;
 	u64 mmio_exit_kernel;
 	u64 exits;
+
+	u64 handle_guest_faults;
+	u64 guest_iabt;
 };
 
 int kvm_vcpu_preferred_target(struct kvm_vcpu_init *init);
diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h
index 6fa70d0e..3fd6288d 100644
--- a/arch/arm64/include/asm/kvm_mmu.h
+++ b/arch/arm64/include/asm/kvm_mmu.h
@@ -180,24 +180,28 @@ void kvm_clear_hyp_idmap(void);
 static inline pte_t kvm_s2pte_mkwrite(pte_t pte)
 {
 	pte_val(pte) |= PTE_S2_RDWR;
+	pte_val(pte) |= PTE_S2_XN;
 	return pte;
 }
 
 static inline pmd_t kvm_s2pmd_mkwrite(pmd_t pmd)
 {
 	pmd_val(pmd) |= PMD_S2_RDWR;
+	pmd_val(pmd) |= PMD_S2_XN;
 	return pmd;
 }
 
 static inline pte_t kvm_s2pte_mkexec(pte_t pte)
 {
 	pte_val(pte) &= ~PTE_S2_XN;
+	pte_val(pte) &= ~PTE_S2_RDWR;
 	return pte;
 }
 
 static inline pmd_t kvm_s2pmd_mkexec(pmd_t pmd)
 {
 	pmd_val(pmd) &= ~PMD_S2_XN;
+	pmd_val(pmd) &= ~PMD_S2_RDWR;
 	return pmd;
 }
 
diff --git a/arch/arm64/include/asm/mmu_context.h b/arch/arm64/include/asm/mmu_context.h
index 39ec0b8a..e9a7db65 100644
--- a/arch/arm64/include/asm/mmu_context.h
+++ b/arch/arm64/include/asm/mmu_context.h
@@ -34,6 +34,7 @@
 #include <asm/pgtable.h>
 #include <asm/sysreg.h>
 #include <asm/tlbflush.h>
+#include <asm/ptdump.h>
 
 static inline void contextidr_thread_switch(struct task_struct *next)
 {
diff --git a/arch/arm64/include/asm/pgtable-prot.h b/arch/arm64/include/asm/pgtable-prot.h
index 5d221620..60386028 100644
--- a/arch/arm64/include/asm/pgtable-prot.h
+++ b/arch/arm64/include/asm/pgtable-prot.h
@@ -69,7 +69,10 @@
 
 #ifdef CONFIG_VERIFIED_KVM
 #define PAGE_S2			__pgprot(_PROT_DEFAULT | PTE_S2_MEMATTR(MT_S2_NORMAL) | PTE_S2_RDONLY)
-#define PAGE_S2_KERNEL		__pgprot(PROT_DEFAULT | PTE_S2_MEMATTR(MT_S2_NORMAL) | PTE_S2_RDWR)
+#define PAGE_S2_XN		__pgprot(_PROT_DEFAULT | PTE_S2_MEMATTR(MT_S2_NORMAL) | PTE_S2_RDWR | PTE_S2_XN)
+#define PAGE_S2_KERNEL		__pgprot(_PROT_DEFAULT | PTE_S2_MEMATTR(MT_S2_NORMAL) | PTE_S2_RDWR)
+#define PAGE_S2_HOST		__pgprot(_PROT_DEFAULT | PTE_S2_MEMATTR(MT_S2_NORMAL) | PTE_S2_RDWR)
+#define PAGE_S2_USER_EXEC	__pgprot(_PROT_DEFAULT | PTE_S2_MEMATTR(MT_S2_NORMAL) | PTE_S2_RDONLY | PTE_PXN)
 #else
 #define PAGE_S2			__pgprot(_PROT_DEFAULT | PTE_S2_MEMATTR(MT_S2_NORMAL) | PTE_S2_RDONLY | PTE_S2_XN)
 #endif
diff --git a/arch/arm64/include/asm/ptdump.h b/arch/arm64/include/asm/ptdump.h
index 6afd8476..4d2a472c 100644
--- a/arch/arm64/include/asm/ptdump.h
+++ b/arch/arm64/include/asm/ptdump.h
@@ -43,12 +43,15 @@ static inline int ptdump_debugfs_register(struct ptdump_info *info,
 }
 #endif
 void ptdump_check_wx(void);
+void ptdump_check_wx_curr(struct task_struct *);
 #endif /* CONFIG_ARM64_PTDUMP_CORE */
 
 #ifdef CONFIG_DEBUG_WX
-#define debug_checkwx()	ptdump_check_wx()
+#define debug_checkwx()		ptdump_check_wx()
+#define debug_checkwx_curr()	ptdump_check_wx_curr(current)
 #else
-#define debug_checkwx()	do { } while (0)
+#define debug_checkwx()		do { } while (0)
+#define debug_checkwx_curr( p )	do { } while (0)
 #endif
 
 #endif /* __ASM_PTDUMP_H */
diff --git a/arch/arm64/kvm/guest.c b/arch/arm64/kvm/guest.c
index ccf30b87..f74db8ed 100644
--- a/arch/arm64/kvm/guest.c
+++ b/arch/arm64/kvm/guest.c
@@ -44,6 +44,8 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	VCPU_STAT(mmio_exit_user),
 	VCPU_STAT(mmio_exit_kernel),
 	VCPU_STAT(exits),
+	VCPU_STAT(handle_guest_faults),
+	VCPU_STAT(guest_iabt),
 	{ NULL }
 };
 
diff --git a/arch/arm64/mm/dump.c b/arch/arm64/mm/dump.c
index 65dfc857..f9c79f8a 100644
--- a/arch/arm64/mm/dump.c
+++ b/arch/arm64/mm/dump.c
@@ -384,6 +384,27 @@ static struct ptdump_info kernel_ptdump_info = {
 	.base_addr	= VA_START,
 };
 
+void ptdump_check_wx_curr(struct task_struct *p)
+{
+	struct pg_state st = {
+		.seq = NULL,
+		.marker = (struct addr_marker[]) {
+			{ 0, NULL},
+			{ -1, NULL},
+		},
+		.check_wx = true,
+	};
+	pr_devel("ptdump_checkwx_curr: %p\n", p);
+
+	walk_pgd(&st, p->mm, p->mm->start_code);
+	note_page(&st, 0, 0, 0);
+	if (st.wx_pages || st.uxn_pages)
+		pr_warn("Checked W+X mappings(p): FAILED, %lu W+X pages found, %lu non-UXN pages found\n",
+			st.wx_pages, st.uxn_pages);
+	else
+		pr_devel("Checked W+X mappings(p): passed, no W+X pages found\n");
+}
+
 void ptdump_check_wx(void)
 {
 	struct pg_state st = {
diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c
index 493ff756..76c187c7 100644
--- a/arch/arm64/mm/mmu.c
+++ b/arch/arm64/mm/mmu.c
@@ -502,8 +502,10 @@ void mark_rodata_ro(void)
 	section_size = (unsigned long)__init_begin - (unsigned long)__start_rodata;
 	update_mapping_prot(__pa_symbol(__start_rodata), (unsigned long)__start_rodata,
 			    section_size, PAGE_KERNEL_RO);
+			    //section_size, __pgprot(PROT_NORMAL & ~PTE_RDONLY | PTE_WRITE & ~PTE_PXN) );
 
 	debug_checkwx();
+	printk("debug_checkwx: passed\n");
 }
 
 static void __init map_kernel_segment(pgd_t *pgdp, void *va_start, void *va_end,
diff --git a/arch/arm64/sekvm/BootAux.c b/arch/arm64/sekvm/BootAux.c
index 0cf26742..76f47bc9 100644
--- a/arch/arm64/sekvm/BootAux.c
+++ b/arch/arm64/sekvm/BootAux.c
@@ -25,7 +25,7 @@ void __hyp_text unmap_and_load_vm_image(u32 vmid, u64 target_addr, u64 remap_add
 		}
 		else
 		{
-			prot_and_map_vm_s2pt(vmid, gfn * PAGE_SIZE, pfn * PAGE_SIZE, 2U);
+			prot_and_map_kernel_s2pt(vmid, gfn * PAGE_SIZE, pfn * PAGE_SIZE, 2U);
 		}
 		start += PMD_SIZE;
 		remap_addr = remap_addr + (start - target_addr);
diff --git a/arch/arm64/sekvm/FaultHandler.c b/arch/arm64/sekvm/FaultHandler.c
index 3dfd1a7f..319f4bf2 100644
--- a/arch/arm64/sekvm/FaultHandler.c
+++ b/arch/arm64/sekvm/FaultHandler.c
@@ -41,6 +41,10 @@ u32 __hyp_text handle_pvops(u32 vmid, u32 vcpuid)
 		{
 			revoke_stage2_sg_gpa(vmid, addr, size);
 		}
+		else if (call_num == KVM_WX_PAGE_MAP)
+		{
+			s2_page_text_wx_map(vmid, addr);
+		}
 		else
 		{
 			ret = 0U;
diff --git a/arch/arm64/sekvm/MemAux.c b/arch/arm64/sekvm/MemAux.c
index ee489497..a1d3230f 100644
--- a/arch/arm64/sekvm/MemAux.c
+++ b/arch/arm64/sekvm/MemAux.c
@@ -29,7 +29,7 @@ void __hyp_text map_page_host(u64 addr)
 	{
 		if (owner == HOSTVISOR || count > 0U)
 		{
-			perm = pgprot_val(PAGE_S2_KERNEL);
+			perm = pgprot_val(PAGE_S2_HOST);
 			new_pte = (pfn * PAGE_SIZE) | perm;
 			mmap_s2pt(HOSTVISOR, addr, 3U, new_pte);
 		}
@@ -54,6 +54,7 @@ void __hyp_text clear_vm_page(u32 vmid, u64 pfn)
 		set_pfn_owner(pfn, HOSTVISOR);
 		set_pfn_count(pfn, 0U);
 		set_pfn_map(pfn, 0UL);
+		reset_pfn_wx(pfn);
 		clear_phys_page(pfn);
 		__flush_dcache_area(__el2_va(pfn << PAGE_SHIFT), PAGE_SIZE);
 	}
@@ -100,6 +101,7 @@ void __hyp_text assign_pfn_to_vm(u32 vmid, u64 gfn, u64 pfn)
 			if (map == INVALID64)
 			{
 				set_pfn_map(pfn, gfn);
+				reset_pfn_wx(pfn);
 			}
 		}
 		else
@@ -118,11 +120,26 @@ void __hyp_text assign_pfn_to_vm(u32 vmid, u64 gfn, u64 pfn)
 
 void __hyp_text map_pfn_vm(u32 vmid, u64 addr, u64 pte, u32 level)
 {
-	u64 paddr, perm;
+	u64 paddr, perm, index;
+	int wx;
 
 	paddr = phys_page(pte);
-	/* We give the VM RWX permission now. */
-	perm = pgprot_val(PAGE_S2_KERNEL);
+
+	/* Set WX based on s2_page.wx */
+	index = get_s2_page_index(paddr);
+	wx = get_s2_page_wx(index);
+	if (wx == 1)
+	{
+		perm = pgprot_val(PAGE_S2_USER_EXEC); // EXEC PXN
+	}
+	else if (wx == 2)
+	{
+		perm = pgprot_val(PAGE_S2_KERNEL); // vm image at load time, UXN
+	}
+	else
+	{
+		perm = pgprot_val(PAGE_S2_XN); // RW, None are executable
+	}
 
 	if (level == 2U)
 	{
diff --git a/arch/arm64/sekvm/MemOps.c b/arch/arm64/sekvm/MemOps.c
index 3cd3adbb..3ff848ef 100644
--- a/arch/arm64/sekvm/MemOps.c
+++ b/arch/arm64/sekvm/MemOps.c
@@ -50,6 +50,43 @@ void __hyp_text prot_and_map_vm_s2pt(u32 vmid, u64 addr, u64 pte, u32 level)
 	map_pfn_vm(vmid, addr, pte, level);
 }
 
+void __hyp_text prot_and_map_kernel_s2pt(u32 vmid, u64 addr, u64 pte, u32 level)
+{
+	u64 pfn, gfn, num, target_addr;
+
+	target_addr = phys_page(pte);
+	pfn = target_addr / PAGE_SIZE;
+	gfn = addr / PAGE_SIZE;
+
+	if (pte == 0)
+	{
+		return;
+	}
+
+	if (level == 2U)
+	{
+		/* gfn is aligned to 2MB size */
+		gfn = gfn / PTRS_PER_PMD * PTRS_PER_PMD;
+		num = PMD_PAGE_NUM;
+		while (num > 0UL)
+		{
+			assign_pfn_to_vm(vmid, gfn, pfn);
+			s2_page_ktext_wx_map(vmid, pfn);
+			gfn += 1UL;
+			pfn += 1UL;
+			num -= 1UL;
+		}
+	}
+	else
+	{
+		assign_pfn_to_vm(vmid, gfn, pfn);
+		s2_page_ktext_wx_map(vmid, pfn);
+		level = 3U;
+	}
+
+	map_pfn_vm(vmid, addr, pte, level);
+}
+
 void __hyp_text grant_stage2_sg_gpa(u32 vmid, u64 addr, u64 size)
 {
 	u32 level;
@@ -119,3 +156,33 @@ void __hyp_text revoke_stage2_sg_gpa(u32 vmid, u64 addr, u64 size)
 		len -= 1UL;
 	}
 }
+
+u64 __hyp_text gpa_to_pfn(u32 vmid, u64 addr)
+{
+	u32 level;
+	u64 pte, pte_pa, pfn=0;
+
+	pte = walk_s2pt(vmid, addr);
+	level = 0;
+	pte_pa = phys_page(pte);
+
+	if (pte & PMD_MARK)
+	{
+		level = 2;
+	}
+	else if (pte & PTE_MARK)
+	{
+		level = 3;
+	}
+
+	if (pte_pa != 0UL)
+	{
+		pfn = pte_pa / PAGE_SIZE;
+		if (level == 2U)
+		{
+			pfn += addr / PAGE_SIZE & 511;
+		}
+	}
+	return pfn;
+}
+
diff --git a/arch/arm64/sekvm/PageMgmt.c b/arch/arm64/sekvm/PageMgmt.c
index 2a724227..1a64c507 100644
--- a/arch/arm64/sekvm/PageMgmt.c
+++ b/arch/arm64/sekvm/PageMgmt.c
@@ -88,3 +88,33 @@ void __hyp_text set_pfn_map(u64 pfn, u64 gfn)
 		set_s2_page_gfn(index, gfn);
 	}
 }
+
+void __hyp_text s2_page_text_wx_map(u32 vmid, u64 gpa)
+{
+	u64 pfn, index;
+	pfn = gpa_to_pfn(vmid, gpa);
+	index = get_s2_page_index(pfn * PAGE_SIZE);
+
+	if ((get_s2_page_gfn(index) == (gpa>>PAGE_SHIFT)) && (get_s2_page_vmid(index) == vmid))
+	{
+		set_s2_page_wx(index, 1);
+	}
+}
+
+void __hyp_text s2_page_ktext_wx_map(u32 vmid, u64 pfn)
+{
+	u64 index;
+	index = get_s2_page_index(pfn * PAGE_SIZE);
+
+	set_s2_page_wx(index, 2);
+}
+
+void __hyp_text reset_pfn_wx(u64 pfn)
+{
+	u64 index;
+	index = get_s2_page_index(pfn * PAGE_SIZE);
+	if (index != INVALID64)
+	{
+		set_s2_page_wx(index, 0);
+	}
+}
diff --git a/arch/arm64/sekvm/hypsec.h b/arch/arm64/sekvm/hypsec.h
index 239c287b..1d6a12bb 100644
--- a/arch/arm64/sekvm/hypsec.h
+++ b/arch/arm64/sekvm/hypsec.h
@@ -253,6 +253,16 @@ static void inline set_s2_page_gfn(u64 index, u64 gfn) {
     el2_data->s2_pages[index].gfn = gfn;
 }
 
+static u64 inline get_s2_page_wx(u64 index) {
+    struct el2_data *el2_data = kern_hyp_va((void*)&el2_data_start);
+    return el2_data->s2_pages[index].wx;
+}
+
+static void inline set_s2_page_wx(u64 index, u64 wx) {
+    struct el2_data *el2_data = kern_hyp_va((void*)&el2_data_start);
+    el2_data->s2_pages[index].wx = wx;
+}
+
 /*
 void    acquire_lock_vm(u32 vmid);
 void    release_lock_vm(u32 vmid);
@@ -793,10 +803,12 @@ void unmap_smmu_page(u32 cbndx, u32 index, u64 iova);
 
 void clear_vm_stage2_range(u32 vmid, u64 start, u64 size);
 void prot_and_map_vm_s2pt(u32 vmid, u64 addr, u64 pte, u32 level);
+void prot_and_map_kernel_s2pt(u32 vmid, u64 addr, u64 pte, u32 level);
 //void grant_stage2_sg_gpa(u32 vmid, u64 addr, u64 size);
 //void revoke_stage2_sg_gpa(u32 vmid, u64 addr, u64 size);
 void map_vm_io(u32 vmid, u64 gpa, u64 pa);
 void clear_vm_range(u32 vmid, u64 pfn, u64 num);
+u64 __hyp_text gpa_to_pfn(u32 vmid, u64 addr);
 
 /*
  * BootCore
diff --git a/drivers/dma-buf/Kconfig b/drivers/dma-buf/Kconfig
index ed3b785b..2e5a0faa 100644
--- a/drivers/dma-buf/Kconfig
+++ b/drivers/dma-buf/Kconfig
@@ -30,4 +30,13 @@ config SW_SYNC
 	  WARNING: improper use of this can result in deadlocking kernel
 	  drivers from userspace. Intended for test and debug only.
 
+config UDMABUF
+	bool "userspace dmabuf misc driver"
+	default n
+	depends on DMA_SHARED_BUFFER
+	depends on MEMFD_CREATE || COMPILE_TEST
+	help
+	  A driver to let userspace turn memfd regions into dma-bufs.
+	  Qemu can use this to create host dmabufs for guest framebuffers.
+
 endmenu
diff --git a/drivers/dma-buf/Makefile b/drivers/dma-buf/Makefile
index c33bf886..0913a6cc 100644
--- a/drivers/dma-buf/Makefile
+++ b/drivers/dma-buf/Makefile
@@ -1,3 +1,4 @@
 obj-y := dma-buf.o dma-fence.o dma-fence-array.o reservation.o seqno-fence.o
 obj-$(CONFIG_SYNC_FILE)		+= sync_file.o
 obj-$(CONFIG_SW_SYNC)		+= sw_sync.o sync_debug.o
+obj-$(CONFIG_UDMABUF)		+= udmabuf.o
diff --git a/drivers/dma-buf/udmabuf.c b/drivers/dma-buf/udmabuf.c
new file mode 100644
index 00000000..3bfd8279
--- /dev/null
+++ b/drivers/dma-buf/udmabuf.c
@@ -0,0 +1,295 @@
+// SPDX-License-Identifier: GPL-2.0
+#include <linux/cred.h>
+#include <linux/device.h>
+#include <linux/dma-buf.h>
+#include <linux/highmem.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/memfd.h>
+#include <linux/miscdevice.h>
+#include <linux/module.h>
+#include <linux/shmem_fs.h>
+#include <linux/slab.h>
+#include <linux/udmabuf.h>
+
+static const u32    list_limit = 1024;  /* udmabuf_create_list->count limit */
+static const size_t size_limit_mb = 64; /* total dmabuf size, in megabytes  */
+
+struct udmabuf {
+	pgoff_t pagecount;
+	struct page **pages;
+};
+
+static int udmabuf_vm_fault(struct vm_fault *vmf)
+{
+	struct vm_area_struct *vma = vmf->vma;
+	struct udmabuf *ubuf = vma->vm_private_data;
+
+	vmf->page = ubuf->pages[vmf->pgoff];
+	get_page(vmf->page);
+	return 0;
+}
+
+static const struct vm_operations_struct udmabuf_vm_ops = {
+	.fault = udmabuf_vm_fault,
+};
+
+static int mmap_udmabuf(struct dma_buf *buf, struct vm_area_struct *vma)
+{
+	struct udmabuf *ubuf = buf->priv;
+
+	if ((vma->vm_flags & (VM_SHARED | VM_MAYSHARE)) == 0)
+		return -EINVAL;
+
+	vma->vm_ops = &udmabuf_vm_ops;
+	vma->vm_private_data = ubuf;
+	return 0;
+}
+
+static struct sg_table *map_udmabuf(struct dma_buf_attachment *at,
+				    enum dma_data_direction direction)
+{
+	struct udmabuf *ubuf = at->dmabuf->priv;
+	struct sg_table *sg;
+	int ret;
+
+	sg = kzalloc(sizeof(*sg), GFP_KERNEL);
+	if (!sg)
+		return ERR_PTR(-ENOMEM);
+	ret = sg_alloc_table_from_pages(sg, ubuf->pages, ubuf->pagecount,
+					0, ubuf->pagecount << PAGE_SHIFT,
+					GFP_KERNEL);
+	if (ret < 0)
+		goto err;
+	if (!dma_map_sg(at->dev, sg->sgl, sg->nents, direction)) {
+		ret = -EINVAL;
+		goto err;
+	}
+	return sg;
+
+err:
+	sg_free_table(sg);
+	kfree(sg);
+	return ERR_PTR(ret);
+}
+
+static void unmap_udmabuf(struct dma_buf_attachment *at,
+			  struct sg_table *sg,
+			  enum dma_data_direction direction)
+{
+	sg_free_table(sg);
+	kfree(sg);
+}
+
+static void release_udmabuf(struct dma_buf *buf)
+{
+	struct udmabuf *ubuf = buf->priv;
+	pgoff_t pg;
+
+	for (pg = 0; pg < ubuf->pagecount; pg++)
+		put_page(ubuf->pages[pg]);
+	kfree(ubuf->pages);
+	kfree(ubuf);
+}
+
+static void *kmap_udmabuf(struct dma_buf *buf, unsigned long page_num)
+{
+	struct udmabuf *ubuf = buf->priv;
+	struct page *page = ubuf->pages[page_num];
+
+	return kmap(page);
+}
+
+static void kunmap_udmabuf(struct dma_buf *buf, unsigned long page_num,
+			   void *vaddr)
+{
+	kunmap(vaddr);
+}
+
+static const struct dma_buf_ops udmabuf_ops = {
+	.map_dma_buf	  = map_udmabuf,
+	.unmap_dma_buf	  = unmap_udmabuf,
+	.release	  = release_udmabuf,
+	.map		  = kmap_udmabuf,
+	.unmap		  = kunmap_udmabuf,
+	.mmap		  = mmap_udmabuf,
+};
+
+#define SEALS_WANTED (F_SEAL_SHRINK)
+#define SEALS_DENIED (F_SEAL_WRITE)
+
+static long udmabuf_create(const struct udmabuf_create_list *head,
+			   const struct udmabuf_create_item *list)
+{
+	DEFINE_DMA_BUF_EXPORT_INFO(exp_info);
+	struct file *memfd = NULL;
+	struct udmabuf *ubuf;
+	struct dma_buf *buf;
+	pgoff_t pgoff, pgcnt, pgidx, pgbuf = 0, pglimit;
+	struct page *page;
+	int seals, ret = -EINVAL;
+	u32 i, flags;
+
+	ubuf = kzalloc(sizeof(*ubuf), GFP_KERNEL);
+	if (!ubuf)
+		return -ENOMEM;
+
+	pglimit = (size_limit_mb * 1024 * 1024) >> PAGE_SHIFT;
+	for (i = 0; i < head->count; i++) {
+		if (!IS_ALIGNED(list[i].offset, PAGE_SIZE))
+			goto err;
+		if (!IS_ALIGNED(list[i].size, PAGE_SIZE))
+			goto err;
+		ubuf->pagecount += list[i].size >> PAGE_SHIFT;
+		if (ubuf->pagecount > pglimit)
+			goto err;
+	}
+	ubuf->pages = kmalloc_array(ubuf->pagecount, sizeof(*ubuf->pages),
+				    GFP_KERNEL);
+	if (!ubuf->pages) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	pgbuf = 0;
+	for (i = 0; i < head->count; i++) {
+		ret = -EBADFD;
+		memfd = fget(list[i].memfd);
+		if (!memfd)
+			goto err;
+		if (!shmem_mapping(file_inode(memfd)->i_mapping))
+			goto err;
+		seals = memfd_fcntl(memfd, F_GET_SEALS, 0);
+		if (seals == -EINVAL)
+			goto err;
+		ret = -EINVAL;
+		if ((seals & SEALS_WANTED) != SEALS_WANTED ||
+		    (seals & SEALS_DENIED) != 0)
+			goto err;
+		pgoff = list[i].offset >> PAGE_SHIFT;
+		pgcnt = list[i].size   >> PAGE_SHIFT;
+		for (pgidx = 0; pgidx < pgcnt; pgidx++) {
+			page = shmem_read_mapping_page(
+				file_inode(memfd)->i_mapping, pgoff + pgidx);
+			if (IS_ERR(page)) {
+				ret = PTR_ERR(page);
+				goto err;
+			}
+			ubuf->pages[pgbuf++] = page;
+		}
+		fput(memfd);
+		memfd = NULL;
+	}
+
+	exp_info.ops  = &udmabuf_ops;
+	exp_info.size = ubuf->pagecount << PAGE_SHIFT;
+	exp_info.priv = ubuf;
+	exp_info.flags = O_RDWR;
+
+	buf = dma_buf_export(&exp_info);
+	if (IS_ERR(buf)) {
+		ret = PTR_ERR(buf);
+		goto err;
+	}
+
+	flags = 0;
+	if (head->flags & UDMABUF_FLAGS_CLOEXEC)
+		flags |= O_CLOEXEC;
+	return dma_buf_fd(buf, flags);
+
+err:
+	while (pgbuf > 0)
+		put_page(ubuf->pages[--pgbuf]);
+	if (memfd)
+		fput(memfd);
+	kfree(ubuf->pages);
+	kfree(ubuf);
+	return ret;
+}
+
+static long udmabuf_ioctl_create(struct file *filp, unsigned long arg)
+{
+	struct udmabuf_create create;
+	struct udmabuf_create_list head;
+	struct udmabuf_create_item list;
+
+	if (copy_from_user(&create, (void __user *)arg,
+			   sizeof(create)))
+		return -EFAULT;
+
+	head.flags  = create.flags;
+	head.count  = 1;
+	list.memfd  = create.memfd;
+	list.offset = create.offset;
+	list.size   = create.size;
+
+	return udmabuf_create(&head, &list);
+}
+
+static long udmabuf_ioctl_create_list(struct file *filp, unsigned long arg)
+{
+	struct udmabuf_create_list head;
+	struct udmabuf_create_item *list;
+	int ret = -EINVAL;
+	u32 lsize;
+
+	if (copy_from_user(&head, (void __user *)arg, sizeof(head)))
+		return -EFAULT;
+	if (head.count > list_limit)
+		return -EINVAL;
+	lsize = sizeof(struct udmabuf_create_item) * head.count;
+	list = memdup_user((void __user *)(arg + sizeof(head)), lsize);
+	if (IS_ERR(list))
+		return PTR_ERR(list);
+
+	ret = udmabuf_create(&head, list);
+	kfree(list);
+	return ret;
+}
+
+static long udmabuf_ioctl(struct file *filp, unsigned int ioctl,
+			  unsigned long arg)
+{
+	long ret;
+
+	switch (ioctl) {
+	case UDMABUF_CREATE:
+		ret = udmabuf_ioctl_create(filp, arg);
+		break;
+	case UDMABUF_CREATE_LIST:
+		ret = udmabuf_ioctl_create_list(filp, arg);
+		break;
+	default:
+		ret = -ENOTTY;
+		break;
+	}
+	return ret;
+}
+
+static const struct file_operations udmabuf_fops = {
+	.owner		= THIS_MODULE,
+	.unlocked_ioctl = udmabuf_ioctl,
+};
+
+static struct miscdevice udmabuf_misc = {
+	.minor          = MISC_DYNAMIC_MINOR,
+	.name           = "udmabuf",
+	.fops           = &udmabuf_fops,
+};
+
+static int __init udmabuf_dev_init(void)
+{
+	pr_notice("udmabuf: init\n");
+	return misc_register(&udmabuf_misc);
+}
+
+static void __exit udmabuf_dev_exit(void)
+{
+	misc_deregister(&udmabuf_misc);
+}
+
+module_init(udmabuf_dev_init)
+module_exit(udmabuf_dev_exit)
+
+MODULE_AUTHOR("Gerd Hoffmann <kraxel@redhat.com>");
+MODULE_LICENSE("GPL v2");
diff --git a/fs/exec.c b/fs/exec.c
index bdd0eace..5795ee07 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -1832,6 +1832,9 @@ static int __do_execve_file(int fd, struct filename *filename,
 		putname(filename);
 	if (displaced)
 		put_files_struct(displaced);
+
+	debug_checkwx_curr();
+
 	return retval;
 
 out:
diff --git a/include/kvm/pvops.h b/include/kvm/pvops.h
index d273a6fc..4657df06 100644
--- a/include/kvm/pvops.h
+++ b/include/kvm/pvops.h
@@ -7,5 +7,6 @@ extern u64 __kvm_call_hyp(void *hypfn, ...);
 #define KVM_SET_DESC_PFN 0x81000
 #define KVM_UNSET_DESC_PFN 0x82000
 #define KVM_SET_BALLOON_PFN 0x83000
+#define KVM_WX_PAGE_MAP 0x84000
 
 #endif /* __KVM_PVOPS_H__ */
diff --git a/include/uapi/linux/udmabuf.h b/include/uapi/linux/udmabuf.h
new file mode 100644
index 00000000..46b6532e
--- /dev/null
+++ b/include/uapi/linux/udmabuf.h
@@ -0,0 +1,33 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+#ifndef _UAPI_LINUX_UDMABUF_H
+#define _UAPI_LINUX_UDMABUF_H
+
+#include <linux/types.h>
+#include <linux/ioctl.h>
+
+#define UDMABUF_FLAGS_CLOEXEC	0x01
+
+struct udmabuf_create {
+	__u32 memfd;
+	__u32 flags;
+	__u64 offset;
+	__u64 size;
+};
+
+struct udmabuf_create_item {
+	__u32 memfd;
+	__u32 __pad;
+	__u64 offset;
+	__u64 size;
+};
+
+struct udmabuf_create_list {
+	__u32 flags;
+	__u32 count;
+	struct udmabuf_create_item list[];
+};
+
+#define UDMABUF_CREATE       _IOW('u', 0x42, struct udmabuf_create)
+#define UDMABUF_CREATE_LIST  _IOW('u', 0x43, struct udmabuf_create_list)
+
+#endif /* _UAPI_LINUX_UDMABUF_H */
diff --git a/virt/kvm/arm/mmu.c b/virt/kvm/arm/mmu.c
index 0cc8f643..68110b85 100644
--- a/virt/kvm/arm/mmu.c
+++ b/virt/kvm/arm/mmu.c
@@ -1827,6 +1827,7 @@ int kvm_handle_guest_abort(struct kvm_vcpu *vcpu, struct kvm_run *run)
 
 	fault_ipa = kvm_vcpu_get_fault_ipa(vcpu);
 	is_iabt = kvm_vcpu_trap_is_iabt(vcpu);
+	vcpu->stat.handle_guest_faults++;
 
 	/* Synchronous External Abort? */
 #ifdef CONFIG_VERIFIED_KVM
@@ -1868,6 +1869,7 @@ int kvm_handle_guest_abort(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	write_fault = kvm_is_write_fault(vcpu);
 	if (kvm_is_error_hva(hva) || (write_fault && !writable)) {
 		if (is_iabt) {
+			vcpu->stat.guest_iabt++;
 #ifndef CONFIG_VERIFIED_KVM
 			/* Prefetch Abort on I/O address, can this happen? */
 			kvm_inject_pabt(vcpu, kvm_vcpu_get_hfar(vcpu));
